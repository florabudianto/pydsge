{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimation Tutorial\n",
    "\n",
    "In this section, we dive into the topic of model estimation using **pydsge**.\n",
    "\n",
    "Now, for this tutorial we will assume a folder set-up of the form\n",
    "\n",
    "```\n",
    "analysis\n",
    "|   README.md\n",
    "|-- src/\n",
    "|  |   estimation.py or .ipynb\n",
    "|  |   model.yaml\n",
    "|-- data/\n",
    "|  |   example_data\n",
    "|-- output/\n",
    "````"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "Just for the tutorial: Setting up example structure\n",
    "import tempfile\n",
    "import os\n",
    "\n",
    "# Temporary output folder\n",
    "output_path = Path(tempfile.gettempdir(), 'analysis/output')\n",
    "os.makedirs(output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsing and loading the model\n",
    "\n",
    "Let us first load the relevant packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path # For Windows/Unix compatibility\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import emcee # For sampling from posterior distribution\n",
    "\n",
    "from pydsge import DSGE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "yaml = \"pydsge_doc/rank.yaml\"\n",
    "# TODO 1: use example model provided with package. Potentially adjust if necessary\n",
    "\n",
    "mod = DSGE.read(yaml)  \n",
    "\n",
    "mod.name = 'rank_test'\n",
    "mod.description = 'RANK, crisis sample'\n",
    "\n",
    "# mod.path = Path(\"pydsge_doc/npz\")\n",
    "mod.path = myTempFolder\n",
    "\n",
    "d0 = pd.read_csv(\n",
    "    Path(\"pydsge_doc/data.csv\"), sep=\";\", index_col=\"date\", parse_dates=True\n",
    ").dropna()\n",
    "# TODO 2: use example data provided with package instead (contains only three time series and no confidential information)\n",
    "\n",
    "# adjust elb\n",
    "zlb = mod.get_par('elb_level')\n",
    "rate = d0['FFR']\n",
    "d0['FFR'] = np.maximum(rate,zlb)\n",
    "\n",
    "mod.load_data(d0, start='1998Q1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing the estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After importing the packages and loading the data, we need to tell pydsge how to carry out the estimation of our model. The \"prep_estim\" method can be used to accomplish this.It can be called without any arguments and sets-up a non-linear model by default. However, to showcase some of this functionality, we decide to specify several arguments here.To perform the estimation, pydsge uses a Transposed-Ensemble Kalman Filter (TEnKF). For general information on its implementation, see [here](https://econsieve.readthedocs.io/en/latest/) , and for more details on running the filter in pydsge check-out the [getting_started tutorial](https://github.com/pcschreiber1/pydsge_OSE_Project_Fork/blob/master/docs/getting_started.ipynb). Again,  the default filter is non-linear, but we can opt for a linear one by setting the argument `Linear` to `True`. To choose a custom number of ensemble members for the TEnKF, set ``N`` to a particular number (default is 300). We can also set a ``seed`` at this point to make the results reproducible. The default value for the seed is set to 0. To get additional information on the process, we can set  ``verbose`` to True. This information includes an overview of the parametersâ€™ distribution, their means and standard deviations. Moreover, if we already specified the covariance matrix of the measurement errors or want to reuse a previous result, we can load it into the prep_estim method by setting ``Load.R=True``. Finally, we can turn parallelization on or off by setting debug to a particular boolean value, which can be helpful in case any issues should arise.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mod.prep_estim(N=350, seed=0, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After finishing our set-up, the only thing left to prepare is to filter our observed FFR for hidden states. We can simply identify the variable through `index` and set the measurement error to a very small value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mod.filter.R = mod.create_obs_cov(1e-1)\n",
    "ind = mod.observables.index('FFR')\n",
    "mod.filter.R[ind,ind] /= 1e1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running the estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the we have all the variables and defined the type of estimation to perform, we can turn to estimating to the model. To be able to deal with very high-dimensional models, `pdygse` uses *Markov Chain Monte Carlo* (MCMC) Integration to sample from the posterior distribution. For further information on MCMC, please refer to the `emcee` [website](https://emcee.readthedocs.io/en/stable/) and the additional resources provided there. We recommend running a **Tempered Ensemble MCMC** first, by using the `tmcmc` method. Doing this is particularly valuable for high-dimensional problems, since defining the initial states of the walkers in the parameterspace in this way is a powerful tool to improve sampling. However, due to its efficiency, we also use it for small models such as the one we are dealing with here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our ensemble sampling, we can specify a variety of options. Note, `tmcmc` always requires the specification of the first four arguments, which are the i) number of steps, ii) number of walks, iii) number of temperatures, and iv) a temperature target! Here we do not want to set a target and, in turn, set `fmax = None`. Moreover, we have the option to set different \"moves\", i.e. coordinate updating algorithms for the walkers. As a wrapper for a lot of `emcee` functionality,  `tmcmc` can work with many different \"moves\" - for a list and implementation details please consult the `emcee` documentation. For using them here, specify them as a list of tuples, containing the type of move and its \"weight\". If no move is specified, \"StretchMove\" is used. For seed setting, the user can choose between three options, here we use the standard numpy seed. Finally, the states are saved in the `p0` object as a numpy array in order to later pass them to our main sampling process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fmax = None\n",
    "\n",
    "moves = [(emcee.moves.DEMove(), 0.8), \n",
    "         (emcee.moves.DESnookerMove(), 0.2),]\n",
    "\n",
    "p0 = mod.tmcmc(200, 200, 0, fmax, moves=moves, update_freq=100, lprob_seed='set')\n",
    "mod.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the output provides us with various important details. In particular, we lean that `mod.save()` saved the meta data of our model in the directory which we specified earlier in `mod.path`. This information is stored as an `.npz` file so that it is avialable even in the event of a crash and can be loaded anytime using `numpy.load()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now use the initial states derived above to conduct our full Bayesian estimation. Still, initial states do not have to be specified and, unless `mcmc` can identify previous runs or estimations, the initial values of the \"prior\" section in the `*.yaml` are used. The default number of sampling steps is 3000, so it makes sense to allow this to run in parallel. However, if you want to avoid this, simply set `debug` to \"True\". And as before, seed setting is essential for creating reproducible results.\n",
    "\n",
    "[*What is purpose of \"tune\", \"update_freq\", \"append\"?*]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod.mcmc(p0,\n",
    "         moves=moves,\n",
    "        #  nsteps=3000,\n",
    "         nsteps = 20,\n",
    "         tune=500,\n",
    "         update_freq=500,\n",
    "         lprob_seed='set',\n",
    "         append=True,\n",
    "         debug=True)\n",
    "mod.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod.__dict__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But, so were are our estimates? Remember that, so far, we have only drawn samples from our posterior distribution. Our converged (burnt-in) MCMC samples are currently stored in the `rank_test_sampler.h5` file created by `mcmc`. To get our parameter estimates, we now still need to draw a sample form the MCMC object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pars = mod.get_par('posterior', nsamples=250, full=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's have a look at the estimated shocks. We can do this by using `extract()` which gives us the smoothed shocks. This method takes a variety of arguments, all of which have sensible default values. For example, here we specify the number of parameter draws in each verification sample to 1. [*is that correct?*]\n",
    "\n",
    " Note also that the default seed is 0, which we simply use here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsd0 = mod.extract(pars, nsamples=1)\n",
    "mod.save_rdict(epsd0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod.mode_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod.mcmc_summary()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d226595e2f076559e618d0a9d30d224ac27d056d5cb4864945e1f27051c61083"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('stud_proj': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
